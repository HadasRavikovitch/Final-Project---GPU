{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HadasRavikovitch/Final-Project---GPU/blob/main/cross_time_tests_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er4kQ7qak7i7"
      },
      "source": [
        "LLG Kernel - Basic function to run on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8rF-m9USex7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float64, int64\n",
        "import math\n",
        "\n",
        "len_matrix = (100000,3)\n",
        "len_M0_norm = 100000\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def cross_product(a, b, result):  # Pass result array as an argument\n",
        "  \"\"\"\n",
        "  Calculates the cross product of two 3D vectors.\n",
        "  \"\"\"\n",
        "  result[0] = a[1] * b[2] - a[2] * b[1]\n",
        "  result[1] = a[2] * b[0] - a[0] * b[2]\n",
        "  result[2] = a[0] * b[1] - a[1] * b[0]\n",
        "  #return result  # No need to return, result is modified in-place\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def norm(array):  # Pass result array as an argument\n",
        "    x = array[0]\n",
        "    y = array[1]\n",
        "    z = array[2]\n",
        "    return math.sqrt(x**2 + y**2 + z**2)\n",
        "\n",
        "@cuda.jit\n",
        "#def LLG_kernel(array1, array2, dt, alpha, llg_result, M_0, an_res, bn_res, cn_res, dn_res):\n",
        "def LLG_kernel(array1, array2, dt, alpha, llg_result):\n",
        "  llg_gama = gama/((1+alpha**2))\n",
        "  llg_lamda = gama*alpha/(1+alpha**2)\n",
        "\n",
        "  cross1 = cuda.local.array(len_matrix, dtype=float64)  # Allocate cross product result arrays\n",
        "  cross2 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cross12 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cross22 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cross13 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cross23 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cross14 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cross24 = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  M_norm = cuda.local.array(len_M0_norm, dtype=np.float64)\n",
        "\n",
        "  an = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  bn = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  cn = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  dn = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "\n",
        "  sum_bn = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  sum_cn = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "  sum_dn = cuda.local.array(len_matrix, dtype=np.float64)\n",
        "\n",
        "  idx = cuda.grid(1)\n",
        "  if idx < array1.shape[0]:\n",
        "    # Calculate M0 (norm) manually\n",
        "    #M_norm[idx] = norm_row(array1[idx])\n",
        "    temp_norm_result = cuda.local.array(1, dtype=float64)\n",
        "    temp_norm_result = norm(array1[idx]) # Call norm_row with two arguments\n",
        "    M_norm[idx] = temp_norm_result\n",
        "\n",
        "    cross_product(array1[idx], array2[idx], cross1[idx])  # Calculate cross products using modified function\n",
        "    cross_product(array1[idx], cross1[idx], cross2[idx])\n",
        "\n",
        "    # Update llg_result directly\n",
        "    # Modify to use element-wise operations:\n",
        "\n",
        "    #an = -gma_LL * miu * np.cross(M, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M, np.cross(M, H, axis=1), axis=1)\n",
        "    #bn = -gma_LL * miu * np.cross(M + (dt / 2) * an, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + (dt / 2) * an, np.cross(M + (dt / 2) * an, H, axis=1), axis=1)\n",
        "    #cn = -gma_LL * miu * np.cross(M + (dt / 2) * bn, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + (dt / 2) * bn, np.cross(M + (dt / 2) * bn, H, axis=1), axis=1)\n",
        "    for i in range(3):\n",
        "        an[idx][i] = -llg_gama * miu * cross1[idx][i] - (llg_lamda * miu / M_norm[idx]) * cross2[idx][i]\n",
        "\n",
        "    for i in range(3):\n",
        "        sum_bn[idx][i] = array1[idx][i] + (dt/2) * an[idx][i]\n",
        "\n",
        "    cross_product(sum_bn[idx], array2[idx], cross12[idx])\n",
        "    cross_product(sum_bn[idx], cross12[idx], cross22[idx])\n",
        "\n",
        "    # Modify to use element-wise operations:\n",
        "    for i in range(3):\n",
        "        bn[idx][i] = -llg_gama * miu * cross12[idx][i] - (llg_lamda * miu / M_norm[idx]) * cross22[idx][i]\n",
        "\n",
        "    for i in range(3):\n",
        "        sum_cn[idx][i] = array1[idx][i] + (dt/2) * bn[idx][i]\n",
        "\n",
        "    cross_product(sum_cn[idx], array2[idx], cross13[idx])\n",
        "    cross_product(sum_cn[idx], cross13[idx], cross23[idx])\n",
        "\n",
        "    for i in range(3):\n",
        "        cn[idx][i] = -llg_gama * miu * cross13[idx][i] - (llg_lamda * miu / M_norm[idx]) * cross23[idx][i]\n",
        "\n",
        "    for i in range(3):\n",
        "        sum_dn[idx][i] = array1[idx][i] + (dt) * cn[idx][i]\n",
        "\n",
        "    cross_product(sum_dn[idx], array2[idx], cross14[idx])\n",
        "    cross_product(sum_dn[idx], cross14[idx], cross24[idx])\n",
        "#   dn = -gma_LL * miu * np.cross(M + dt * cn, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + dt * cn, np.cross(M + dt * cn, H, axis=1), axis=1)\n",
        "\n",
        "    for i in range(3):\n",
        "        dn[idx][i] = -llg_gama * miu * cross14[idx][i] - (llg_lamda * miu / M_norm[idx]) * cross24[idx][i]\n",
        "\n",
        "    for i in range(3):\n",
        "      llg_result[idx][i] = array1[idx][i] + (dt/6)*(an[idx][i] + 2*bn[idx][i] + 2*cn[idx][i] + dn[idx][i])\n",
        "\n",
        "    # FOR DEBUG:\n",
        "    #for i in range(3):\n",
        "      #an_res[idx][i] = an[idx][i]\n",
        "      #bn_res[idx][i] = bn[idx][i]\n",
        "      #cn_res[idx][i] = cn[idx][i]\n",
        "      #dn_res[idx][i] = dn[idx][i]\n",
        "    #M_0[idx] = M_norm[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf032hXSlHlV"
      },
      "source": [
        "Running the LLG KERNEL for short input arrays (len 3)\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "JLWqZmOow4Ch",
        "outputId": "c68adbdf-a82e-4aaf-b329-3600ab95ee11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Minor version compatibility requires ptxcompiler and cubinlinker packages to be available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_registers, lineinfo, cc)\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mA\u001b[0m \u001b[0mLinkableCode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m         \u001b[0mobject\u001b[0m \u001b[0mrepresents\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[0malready\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cubinlinker'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7ec081852543>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#LLG_kernel[blocks, threadsperblock](d_x, d_y, dt, alpha, d_res, d_M_norm, d_an_res, d_bn_res, d_cn_res, d_dn_res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mLLG_kernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadsperblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"res:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#print(\"M0:\",d_M_norm.copy_to_host())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    537\u001b[0m             raise ValueError(\"Can't create ForAll with negative task count: %s\"\n\u001b[1;32m    538\u001b[0m                              % ntasks)\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread_per_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_numba_type_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcuda_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDADispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menable_caching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_kernel_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_LaunchConfiguration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0msignature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0monce\u001b[0m \u001b[0mby\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0minside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0mthis\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mCompileResult\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnrt_in_asm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mnrt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'runtime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nrt.cu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\u001b[0m in \u001b[0;36mget_cufunc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# `-ptx` flag is meant to view the optimized PTX for LTO objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# Non-LTO objects are not passed to linker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_nonlto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mptx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_linked_ptx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\u001b[0m in \u001b[0;36mget_cubin\u001b[0;34m(self, cc)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ltoir_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mltoir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mltoir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_link_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_nonlto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(cls, max_registers, lineinfo, cc)\u001b[0m\n\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m     def new(cls,\n\u001b[0m\u001b[1;32m   2577\u001b[0m             \u001b[0mmax_registers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m             \u001b[0mlineinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_registers, lineinfo, cc)\u001b[0m\n\u001b[1;32m   2665\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0mrepresents\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[0malready\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m         \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mignore_nonlto\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madd\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0mLTO\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0med\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlinking\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0museful\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspecting\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0mLTO\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0med\u001b[0m \u001b[0mportion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPTX\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mlinker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0madded\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Minor version compatibility requires ptxcompiler and cubinlinker packages to be available",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float64, int64\n",
        "import math\n",
        "\n",
        "#physical parameters\n",
        "eps=8.854e-12 #[F/m]\n",
        "miu=4*np.pi*1e-7 #[H/m]\n",
        "c=1/(eps*miu)**0.5\n",
        "heta = (miu/eps)**0.5\n",
        "q = 1.60217646e-19    # Elementary charge [Coulombs]\n",
        "miu = 4 * np.pi * 1e-7    # Magnetic permeability [H/m]\n",
        "g = 2    # Landau factor\n",
        "me = 9.1093821545e-31    # Electron mass [kg]\n",
        "gma_factor = 1\n",
        "gama = gma_factor * g * q / (2 * me)\n",
        "alpha = 0\n",
        "\n",
        "dz = 2e-9/8\n",
        "dt = 2\n",
        "\n",
        "x = np.array([[1.,2.,3.], [4.,5.,6.], [7.,2.,5.]], dtype = np.float64)\n",
        "#norm_x = np.array(np.linalg.norm(x, axis=1))\n",
        "y = np.array([[4,5,6], [1,7,3], [4,5,6]], dtype = np.float64)\n",
        "M_norm = np.arange(3, dtype=np.float64)\n",
        "#an_res = np.arange(3, dtype=np.float64)\n",
        "#d_x = cuda.to_device(x)\n",
        "#d_y = cuda.to_device(y)\n",
        "res = np.empty_like(x)\n",
        "d_x = cuda.to_device(x)\n",
        "d_y = cuda.to_device(y)\n",
        "d_res = cuda.device_array_like(d_x)\n",
        "\n",
        "# FOR DEBUG - uncomment and print to understand better:\n",
        "#d_an_res = cuda.device_array_like(d_x)\n",
        "#d_bn_res = cuda.device_array_like(d_x)\n",
        "#d_cn_res = cuda.device_array_like(d_x)\n",
        "#d_dn_res = cuda.device_array_like(d_x)\n",
        "#d_M_norm = cuda.device_array_like(M_norm)\n",
        "\n",
        "blocks = 8  # Ensure enough blocks to cover the data\n",
        "threadsperblock = 64 # Ensure enough threads to cover the data\n",
        "\n",
        "# Call the kernel with launch configuration\n",
        "# FOR DEBUG VERSION (with M0, an, bn...):\n",
        "#LLG_kernel[blocks, threadsperblock](d_x, d_y, dt, alpha, d_res, d_M_norm, d_an_res, d_bn_res, d_cn_res, d_dn_res)\n",
        "\n",
        "LLG_kernel[blocks, threadsperblock](d_x, d_y, dt, alpha, d_res)\n",
        "print(\"res:\",d_res.copy_to_host())\n",
        "#print(\"M0:\",d_M_norm.copy_to_host())\n",
        "#print(\"an:\",d_an_res.copy_to_host())\n",
        "#print(\"bn:\",d_bn_res.copy_to_host())\n",
        "#print(\"cn:\",d_cn_res.copy_to_host())\n",
        "#print(\"dn:\",d_dn_res.copy_to_host())"
      ]
    },
    {
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYvbNwxyECkS",
        "outputId": "f3a7a4f2-e296-40a4-e711-d7a2b283a9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 16 09:21:42 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   61C    P0             30W /   72W |     215MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install --upgrade numba-cuda"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6GB-PhMERe5",
        "outputId": "aea98bda-1c44-4a54-d101-ffd0042f0274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba-cuda in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numba>=0.59.1 in /usr/local/lib/python3.11/dist-packages (from numba-cuda) (0.61.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59.1->numba-cuda) (0.44.0)\n",
            "Requirement already satisfied: numpy<2.2,>=1.24 in /usr/local/lib/python3.11/dist-packages (from numba>=0.59.1->numba-cuda) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0vZL2Cr3d4d",
        "outputId": "1bc8000d-8973-4d6b-d83d-45bc5d1eb394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67.7 µs ± 661 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit LLG_kernel[4,16](d_x, d_y, dt, alpha, d_res); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -q --system --force-reinstall numba-cuda==0.4.0"
      ],
      "metadata": {
        "id": "d6lg3WALiHNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsEzUeNWnjVY"
      },
      "outputs": [],
      "source": [
        "from numba import config\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import config\n",
        "config.CUDA_ENABLE_MINOR_VERSION_COMPATIBILITY = True"
      ],
      "metadata": {
        "id": "KWg8aVaPMAry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJStAI9flRxS"
      },
      "source": [
        "LLG Step - The basic function to run on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vro-bkVrO8i3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def LLG_step(M: np.array, H: np.array, dt: float, alpha: float) -> np.array:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    M0 = np.linalg.norm(M, axis=1, keepdims=True)\n",
        "    #print(\"M0:\", M0)\n",
        "    gma_LL=gama/((1+alpha**2))\n",
        "    LL_lambda=gama*alpha/(1+alpha**2)\n",
        "\n",
        "   # Compute LLG terms\n",
        "    an = -gma_LL * miu * np.cross(M, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M, np.cross(M, H, axis=1), axis=1)\n",
        "    #print(\"an:\",an)\n",
        "    bn = -gma_LL * miu * np.cross(M + (dt / 2) * an, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + (dt / 2) * an, np.cross(M + (dt / 2) * an, H, axis=1), axis=1)\n",
        "    #print(\"bn:\",bn)\n",
        "    cn = -gma_LL * miu * np.cross(M + (dt / 2) * bn, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + (dt / 2) * bn, np.cross(M + (dt / 2) * bn, H, axis=1), axis=1)\n",
        "    #print(\"cn:\",cn)\n",
        "    dn = -gma_LL * miu * np.cross(M + dt * cn, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + dt * cn, np.cross(M + dt * cn, H, axis=1), axis=1)\n",
        "    #print(\"dn:\",dn)\n",
        "    new_M = M + (dt/6)*(an+2*bn+2*cn+dn)\n",
        "    return new_M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mapby7SlW_J"
      },
      "source": [
        "Running the LLG on CPU for short input arrays (len 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbSUd8U9Pkcl",
        "outputId": "bc88b1ed-540c-4001-e51b-00475b284006"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-6.24733836e+24, -7.34973942e+23,  4.77737052e+24],\n",
              "       [ 1.68011243e+25, -9.76156552e+24,  1.71766114e+25],\n",
              "       [ 3.27066219e+25, -2.27844227e+25, -2.81739568e+24]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "\n",
        "#physical parameters\n",
        "eps=8.854e-12 #[F/m]\n",
        "miu=4*np.pi*1e-7 #[H/m]\n",
        "c=1/(eps*miu)**0.5\n",
        "heta = (miu/eps)**0.5\n",
        "q = 1.60217646e-19    # Elementary charge [Coulombs]\n",
        "miu = 4 * np.pi * 1e-7    # Magnetic permeability [H/m]\n",
        "g = 2    # Landau factor\n",
        "me = 9.1093821545e-31    # Electron mass [kg]\n",
        "gma_factor = 1\n",
        "gama = gma_factor * g * q / (2 * me)\n",
        "alpha = 0\n",
        "\n",
        "dz = 2e-9/8\n",
        "dt = 2\n",
        "\n",
        "x = np.array([[1.,2.,3.], [4.,5.,6.], [7.,2.,5.]], dtype = np.float64)\n",
        "y = np.array([[4,5,6], [1,7,3], [4,5,6]], dtype = np.float64)\n",
        "#d_x = cuda.to_device(x)\n",
        "#d_y = cuda.to_device(y)\n",
        "res = np.empty_like(x)\n",
        "\n",
        "LLG_step(x, y, dt, alpha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83xrcb4EvVC_",
        "outputId": "6961caf1-3811-428a-bb4c-8cdba2adb622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "363 µs ± 39.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit LLG_step(x, y, dt, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atpvI6lZleIi"
      },
      "source": [
        "Adding the time propogate loop - Where in each iteration there is a call for the LLG Kernel. Suppose to be less time effictiate beacuse in each iterate there is a data transfer for and to host and device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIrPphVBvjhZ",
        "outputId": "9eb71f55-ceda-4bb9-e867-501090e2c0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104 µs ± 12.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit LLG_kernel[blocks,threadsperblock](d_big_arr1, d_big_arr2, dt, alpha, d_res); cuda.synchronize()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOMbsTdrGsMngWpEcL35vuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}