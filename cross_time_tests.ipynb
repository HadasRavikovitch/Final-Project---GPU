{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HadasRavikovitch/Final-Project---GPU/blob/main/cross_time_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLWqZmOow4Ch",
        "outputId": "a1ae2451-cdac-4859-8766-0b450975b246"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-3.12367001e+24 -3.67485308e+23  2.38868443e+24]\n",
            " [ 8.40055643e+24 -4.88078403e+24  8.58831060e+24]\n",
            " [ 1.63533074e+25 -1.13922175e+25 -1.40869036e+24]]\n",
            "[3.74165739 3.74165739 3.74165739]\n",
            "[[  663059.56357368   663059.56357368   663059.56357368]\n",
            " [-1326119.12714735 -1326119.12714735 -1326119.12714735]\n",
            " [  663059.56357368   663059.56357368   663059.56357368]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "\n",
        "#physical parameters\n",
        "eps=8.854e-12 #[F/m]\n",
        "miu=4*np.pi*1e-7 #[H/m]\n",
        "c=1/(eps*miu)**0.5\n",
        "heta = (miu/eps)**0.5\n",
        "q = 1.60217646e-19    # Elementary charge [Coulombs]\n",
        "miu = 4 * np.pi * 1e-7    # Magnetic permeability [H/m]\n",
        "g = 2    # Landau factor\n",
        "me = 9.1093821545e-31    # Electron mass [kg]\n",
        "gma_factor = 1\n",
        "gama = gma_factor * g * q / (2 * me)\n",
        "alpha = 0\n",
        "\n",
        "dz = 2e-9/8\n",
        "dt = 2\n",
        "\n",
        "x = np.array([[1.,2.,3.], [4.,5.,6.], [7.,2.,5.]], dtype = np.float64)\n",
        "#norm_x = np.array(np.linalg.norm(x, axis=1))\n",
        "y = np.array([[4,5,6], [1,7,3], [4,5,6]], dtype = np.float64)\n",
        "M_norm = np.arange(3, dtype=np.float64)\n",
        "#an_res = np.arange(3, dtype=np.float64)\n",
        "#d_x = cuda.to_device(x)\n",
        "#d_y = cuda.to_device(y)\n",
        "res = np.empty_like(x)\n",
        "d_x = cuda.to_device(x)\n",
        "d_y = cuda.to_device(y)\n",
        "d_res = cuda.device_array_like(d_x)\n",
        "d_M_norm = cuda.device_array_like(M_norm)\n",
        "\n",
        "d_an_res = cuda.device_array_like(d_x)\n",
        "d_bn_res = cuda.device_array_like(d_x)\n",
        "d_cn_res = cuda.device_array_like(d_x)\n",
        "d_dn_res = cuda.device_array_like(d_x)\n",
        "\n",
        "blocks = 4  # Ensure enough blocks to cover the data\n",
        "threadsperblock = 32 # Ensure enough threads to cover the data\n",
        "\n",
        "# Call the kernel with launch configuration\n",
        "#cross[blocks, threadsperblock](d_x, d_y, d_cross)\n",
        "#print(d_cross.copy_to_host())\n",
        "LLG_kernel[blocks, threadsperblock](d_x, d_y, dt, alpha, d_res, d_M_norm, d_an_res, d_bn_res, d_cn_res, d_dn_res)\n",
        "cuda.synchronize()\n",
        "print(d_res.copy_to_host())\n",
        "print(d_M_norm.copy_to_host())\n",
        "print(d_an_res.copy_to_host())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8rF-m9USex7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float64, int64\n",
        "import math\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def cross_product(a, b, result):  # Pass result array as an argument\n",
        "  \"\"\"\n",
        "  Calculates the cross product of two 3D vectors.\n",
        "  \"\"\"\n",
        "  #result = cuda.local.array(3, dtype=np.float64)  # Remove local allocation\n",
        "  result[0] = a[1] * b[2] - a[2] * b[1]\n",
        "  result[1] = a[2] * b[0] - a[0] * b[2]\n",
        "  result[2] = a[0] * b[1] - a[1] * b[0]\n",
        "  #return result  # No need to return, result is modified in-place\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def norm(array):  # Pass result array as an argument\n",
        "  norm = math.sqrt(array[0]**2 + array[1]**2 + array[2]**2)\n",
        "  return norm\n",
        "\n",
        "@cuda.jit\n",
        "def LLG_kernel(array1, array2, dt, alpha, llg_result, M_0, an_res, bn_res, cn_res, dn_res):\n",
        "  llg_gama = gama/((1+alpha**2))\n",
        "  llg_lamda = gama*alpha/(1+alpha**2)\n",
        "  M_norm = cuda.local.array(array1.shape[0], dtype=np.float64)\n",
        "\n",
        "  idx = cuda.grid(1)\n",
        "  if idx < array1.shape[0]:\n",
        "    # Calculate M0 (norm) manually\n",
        "    M_norm[id] = norm(array1[idx])\n",
        "    cross1 = cuda.local.array(3, dtype=np.float64)  # Allocate cross product result arrays\n",
        "    cross2 = cuda.local.array(3, dtype=np.float64)\n",
        "    cross12 = cuda.local.array(3, dtype=np.float64)\n",
        "    cross22 = cuda.local.array(3, dtype=np.float64)\n",
        "    cross13 = cuda.local.array(3, dtype=np.float64)\n",
        "    cross23 = cuda.local.array(3, dtype=np.float64)\n",
        "    cross14 = cuda.local.array(3, dtype=np.float64)\n",
        "    cross24 = cuda.local.array(3, dtype=np.float64)\n",
        "\n",
        "    an = cuda.local.array(3, dtype=np.float64)\n",
        "    bn = cuda.local.array(3, dtype=np.float64)\n",
        "    cn = cuda.local.array(3, dtype=np.float64)\n",
        "    dn = cuda.local.array(3, dtype=np.float64)\n",
        "\n",
        "    sum_bn = cuda.local.array(3, dtype=np.float64)\n",
        "    sum_cn = cuda.local.array(3, dtype=np.float64)\n",
        "    sum_dn = cuda.local.array(3, dtype=np.float64)\n",
        "\n",
        "    cross_product(array1[idx], array2[idx], cross1)  # Calculate cross products using modified function\n",
        "    cross_product(array1[idx], cross1, cross2)\n",
        "\n",
        "    # Update llg_result directly\n",
        "    # Modify to use element-wise operations:\n",
        "    for i in range(3):\n",
        "        an[i] = -llg_gama * miu * cross1[i] - (llg_lamda * miu / M_norm) * cross2[i]\n",
        "\n",
        "    for i in range(3):\n",
        "        sum_bn[i] = array1[idx][i] + (dt/2) * an[i]\n",
        "\n",
        "    cross_product(sum_bn, array2[idx], cross12)\n",
        "    cross_product(sum_bn, cross12, cross22)\n",
        "\n",
        "    # Modify to use element-wise operations:\n",
        "    for i in range(3):\n",
        "        bn[i] = -llg_gama * miu * cross12[i] - (llg_lamda * miu / M_norm) * cross22[i]\n",
        "\n",
        "    for i in range(3):\n",
        "        sum_cn[i] = array1[idx][i] + (dt/2) * bn[i]\n",
        "\n",
        "    cross_product(sum_cn, array2[idx], cross13)\n",
        "    cross_product(sum_cn, cross13, cross23)\n",
        "\n",
        "    for i in range(3):\n",
        "        cn[i] = -llg_gama * miu * cross13[i] - (llg_lamda * miu / M_norm) * cross23[i]\n",
        "\n",
        "    for i in range(3):\n",
        "        sum_dn[i] = array1[idx][i] + (dt/2) * cn[i]\n",
        "\n",
        "    cross_product(sum_dn, array2[idx], cross14)\n",
        "    cross_product(sum_dn, cross14, cross24)\n",
        "\n",
        "    for i in range(3):\n",
        "        dn[i] = -llg_gama * miu * cross14[i] - (llg_lamda * miu / M_norm) * cross24[i]\n",
        "\n",
        "    for i in range(3):\n",
        "      llg_result[idx][i] = array1[idx][i] + (dt/6)*(an[i] + 2*bn[i] + 2*cn[i] + dn[i])\n",
        "\n",
        "    for i in range(3):\n",
        "      an_res[i] = an[i]\n",
        "      M_0[i] = M_norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0vZL2Cr3d4d",
        "outputId": "52fc5d8f-7ef2-4d87-8b1e-2f546bfd683c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94.4 µs ± 20.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit LLG_kernel[4,16](d_x, d_y, dt, alpha, d_res); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vro-bkVrO8i3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def LLG_step(M: np.array, H: np.array, dt: float, alpha: float) -> np.array:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    M0 = np.linalg.norm(M, axis=1, keepdims=True)\n",
        "    print(\"M0:\", M0)\n",
        "    gma_LL=gama/((1+alpha**2))\n",
        "    LL_lambda=gama*alpha/(1+alpha**2)\n",
        "\n",
        "   # Compute LLG terms\n",
        "    an = -gma_LL * miu * np.cross(M, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M, np.cross(M, H, axis=1), axis=1)\n",
        "    print(an)\n",
        "    bn = -gma_LL * miu * np.cross(M + (dt / 2) * an, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + (dt / 2) * an, np.cross(M + (dt / 2) * an, H, axis=1), axis=1)\n",
        "    print(bn)\n",
        "    cn = -gma_LL * miu * np.cross(M + (dt / 2) * bn, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + (dt / 2) * bn, np.cross(M + (dt / 2) * bn, H, axis=1), axis=1)\n",
        "    print(cn)\n",
        "    dn = -gma_LL * miu * np.cross(M + dt * cn, H, axis=1) - (LL_lambda * miu / M0) * np.cross(M + dt * cn, np.cross(M + dt * cn, H, axis=1), axis=1)\n",
        "    print(dn)\n",
        "    new_M = M + (dt/6)*(an+2*bn+2*cn+dn)\n",
        "    return new_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbSUd8U9Pkcl",
        "outputId": "bbb15d35-10dc-4a61-f8c9-5401faa40393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M0: [[3.74165739]\n",
            " [8.77496439]\n",
            " [8.83176087]]\n",
            "[[  663059.56357368 -1326119.12714735   663059.56357368]\n",
            " [ 5967536.07216308  1326119.12714735 -5083456.65406485]\n",
            " [ 2873258.10881926  4862436.79954029 -5967536.07216308]]\n",
            "[[ 2.49133924e+12  2.93097330e+11 -1.90514060e+12]\n",
            " [-8.74410395e+12  5.08037804e+12 -8.93951411e+12]\n",
            " [-1.30428873e+13  9.08606322e+12  1.12353888e+12]]\n",
            "[[-2.49405147e+18  4.98810822e+18 -2.49405587e+18]\n",
            " [-1.71992640e+19 -3.82205164e+18  1.46512085e+19]\n",
            " [-1.08075802e+19 -1.82897200e+19  2.24464868e+19]]\n",
            "[[-1.87420101e+25 -2.20493180e+24  1.43321166e+25]\n",
            " [ 5.04034074e+25 -2.92846889e+25  5.15298050e+25]\n",
            " [ 9.81198875e+25 -6.83532317e+25 -8.45223192e+24]]\n",
            "[[-6.24733836e+24 -7.34973942e+23  4.77737052e+24]\n",
            " [ 1.68011243e+25 -9.76156552e+24  1.71766114e+25]\n",
            " [ 3.27066219e+25 -2.27844227e+25 -2.81739568e+24]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "\n",
        "#physical parameters\n",
        "eps=8.854e-12 #[F/m]\n",
        "miu=4*np.pi*1e-7 #[H/m]\n",
        "c=1/(eps*miu)**0.5\n",
        "heta = (miu/eps)**0.5\n",
        "q = 1.60217646e-19    # Elementary charge [Coulombs]\n",
        "miu = 4 * np.pi * 1e-7    # Magnetic permeability [H/m]\n",
        "g = 2    # Landau factor\n",
        "me = 9.1093821545e-31    # Electron mass [kg]\n",
        "gma_factor = 1\n",
        "gama = gma_factor * g * q / (2 * me)\n",
        "alpha = 0\n",
        "\n",
        "dz = 2e-9/8\n",
        "dt = 2\n",
        "\n",
        "x = np.array([[1.,2.,3.], [4.,5.,6.], [7.,2.,5.]], dtype = np.float64)\n",
        "y = np.array([[4,5,6], [1,7,3], [4,5,6]], dtype = np.float64)\n",
        "#d_x = cuda.to_device(x)\n",
        "#d_y = cuda.to_device(y)\n",
        "res = np.empty_like(x)\n",
        "\n",
        "print(LLG_step(x, y, dt, alpha))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83xrcb4EvVC_",
        "outputId": "2d20b067-9826-482f-e7d5-b95ff0554db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "508 µs ± 94.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit LLG_step(x, y, dt, alpha)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM28vWY3mMCrEhOkMak2D/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}